"use strict";(globalThis.webpackChunkrobolearn=globalThis.webpackChunkrobolearn||[]).push([[7957],{4861:(n,o,e)=>{e.r(o),e.d(o,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-humanoid-vla/conversational-robotics/index","title":"Chapter 20: Conversational Robotics & VLAs","description":"This chapter brings together the power of Large Language Models (LLMs) with robot perception and action. You will learn to build Vision-Language-Action (VLA) systems that allow a robot to understand natural language commands and translate them into physical actions.","source":"@site/docs/module-4-humanoid-vla/20-conversational-robotics/index.md","sourceDirName":"module-4-humanoid-vla/20-conversational-robotics","slug":"/module-4-humanoid-vla/conversational-robotics/","permalink":"/hackathon/docs/module-4-humanoid-vla/conversational-robotics/","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammadsaimraza/docs/module-4-humanoid-vla/20-conversational-robotics/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Lesson 19.2: Introduction to MoveIt2","permalink":"/hackathon/docs/module-4-humanoid-vla/motion-planning/19-2-introduction-to-moveit2"},"next":{"title":"Lesson 20.1: The Voice-to-Action Pipeline","permalink":"/hackathon/docs/module-4-humanoid-vla/conversational-robotics/20-1-voice-to-action-pipeline"}}');var i=e(4848),a=e(8453);const s={},l="Chapter 20: Conversational Robotics & VLAs",r={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Lessons",id:"lessons",level:2}];function d(n){const o={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.header,{children:(0,i.jsx)(o.h1,{id:"chapter-20-conversational-robotics--vlas",children:"Chapter 20: Conversational Robotics & VLAs"})}),"\n",(0,i.jsxs)(o.p,{children:["This chapter brings together the power of Large Language Models (LLMs) with robot perception and action. You will learn to build ",(0,i.jsx)(o.strong,{children:"Vision-Language-Action (VLA)"})," systems that allow a robot to understand natural language commands and translate them into physical actions."]}),"\n",(0,i.jsx)(o.p,{children:'This is the cutting edge of robotics, where AI\'s ability to reason about the world through language meets its ability to operate within it. You will learn how to build a complete "Voice-to-Action" pipeline, enabling your robot to respond intelligently to human commands.'}),"\n",(0,i.jsx)(o.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(o.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Explain the components of a Voice-to-Action pipeline (Speech-to-Text, LLM Planning, Plan-to-Action)."}),"\n",(0,i.jsx)(o.li,{children:"Use an LLM for cognitive planning, translating natural language into structured robot plans."}),"\n",(0,i.jsx)(o.li,{children:"Understand the challenges and opportunities of multi-modal interaction in robotics."}),"\n"]}),"\n",(0,i.jsx)(o.h2,{id:"lessons",children:"Lessons"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:(0,i.jsx)(o.strong,{children:"Lesson 20.1: The Voice-to-Action Pipeline"})}),"\n",(0,i.jsx)(o.li,{children:(0,i.jsx)(o.strong,{children:"Lesson 20.2: LLM Cognitive Planning"})}),"\n"]})]})}function h(n={}){const{wrapper:o}={...(0,a.R)(),...n.components};return o?(0,i.jsx)(o,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,o,e)=>{e.d(o,{R:()=>s,x:()=>l});var t=e(6540);const i={},a=t.createContext(i);function s(n){const o=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(o):{...o,...n}},[o,n])}function l(n){let o;return o=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(a.Provider,{value:o},n.children)}}}]);