# سبق 20.2: LLM کوگنیٹیو پلاننگ

وائس-ٹو-ایکشن پائپ لائن کا دل **LLM کوگنیٹیو پلانر** ہے۔ یہ وہ جگہ ہے جہاں روبوٹ انسان کے ارادے کو حقیقی معنوں میں "سمجھتا ہے" اور اسے قابل عمل روبوٹ اعمال کے سلسلے میں ترجمہ کرتا ہے۔

ہم Large Language Model (LLM) کو روبوٹ کے اعلیٰ سطحی دماغ کے طور پر دیکھ رہے ہیں۔ یہ انفرادی جوائنٹ حرکات کو کنٹرول نہیں کر رہا ہے، بلکہ اعلیٰ سطحی مہارتوں کے سلسلے کو منظم کر رہا ہے۔

## پرامپٹ انجینئرنگ کا کردار

مؤثر LLM کوگنیٹیو پلاننگ کی کلید **پرامپٹ انجینئرنگ** ہے۔ آپ کو LLM کو اس کے کردار، ان اعمال جو وہ انجام دے سکتا ہے، اور مطلوبہ آؤٹ پٹ فارمیٹ کے بارے میں واضح ہدایات دینے کی ضرورت ہے۔

ایک اچھی طرح سے ڈیزائن کیا گیا پرامپٹ عام طور پر شامل کرے گا:
1.  **کردار کی تعریف:** "آپ ایک سروس روبوٹ کے لیے پلاننگ کا دماغ ہیں۔"
2.  **دستیاب اعمال:** روبوٹ کی اعلیٰ سطحی مہارتوں کی ایک فہرست، ان کے پیرامیٹرز کے ساتھ۔
    *   `GOTO(location_name)`
    *   `FIND(object_name)`
    *   `PICKUP(object_name)`
    *   `DELIVER(object_name, person_name)`
3.  **معلوم سیاق و سباق:** ماحول کے بارے میں معلومات (مثلاً، "معلوم مقامات ہیں: کچن، لیونگ روم، بیڈروم۔ معلوم اشیاء ہیں: پانی کی بوتل، کتاب، چابیاں۔")
4.  **آؤٹ پٹ فارمیٹ:** "اپنا پلان JSON فارمیٹ میں آؤٹ پٹ کریں۔"
5.  **مثال گفتگو کا موڑ:** انسانی کمانڈز اور متوقع روبوٹ پلان کی چند مثالیں۔

## مثال پرامپٹ

```text
آپ Rosie نامی ایک گھریلو سروس روبوٹ کے لیے پلاننگ کا دماغ ہیں۔ آپ کا مقصد صارف کی درخواستوں کو پورا کرنے کے لیے JSON فارمیٹ میں ایک سٹرکچرڈ پلان تیار کرنا ہے۔

دستیاب اعمال:
- GOTO(مقام: str): ایک مخصوص مقام پر جائیں۔ درست مقامات: 'کچن'، 'لیونگ روم'، 'بیڈروم'، 'آفس'۔
- FIND(آبجیکٹ کا نام: str): موجودہ کمرے میں بصری پرسیپشن کا استعمال کرتے ہوئے ایک مخصوص آبجیکٹ کو تلاش کریں۔
- PICKUP(آبجیکٹ کا نام: str): ایک مخصوص آبجیکٹ اٹھائیں۔ فرض کیا جاتا ہے کہ روبوٹ آبجیکٹ کی جگہ پر ہے۔
- DELIVER(آبجیکٹ کا نام: str، وصول کنندہ کا نام: str): ایک شخص کو ایک آبجیکٹ پہنچائیں۔ درست وصول کنندگان: 'صارف'، 'مہمان'۔

روبوٹ کا موجودہ مقام: 'لیونگ روم'

صارف کا کمانڈ: "Rosie، کیا آپ براہ کرم مجھے کچن سے پانی کی بوتل لا سکتے ہیں؟"

اپنا پلان ایکشن آبجیکٹس کے JSON ایرے کے طور پر آؤٹ پٹ کریں۔

مثال:
صارف: "کچن میں جاؤ۔"
Rosie:
[
  {"action": "GOTO", "parameters": {"location": "kitchen"}}
]
```

## LLM آؤٹ پٹ

اگر صحیح طریقے سے پرامپٹ کیا جائے، تو LLM پھر اس طرح ایک JSON پلان آؤٹ پٹ کرے گا:

```json
[
  {"action": "GOTO", "parameters": {"location": "kitchen"}},
  {"action": "FIND", "parameters": {"object_name": "water_bottle"}},
  {"action": "PICKUP", "parameters": {"object_name": "water_bottle"}},
  {"action": "GOTO", "parameters": {"location": "user"}},
  {"action": "DELIVER", "parameters": {"object_name": "water_bottle", "recipient_name": "user"}}
]
```
یہ سٹرکچرڈ JSON پھر "پلان-ٹو-ایکشن" ایگزیکیوشن ماڈیول کو پاس کیا جاتا ہے، جو ان اعلیٰ سطحی اعمال کو ROS 2 ایکشن سرورز اور سروس کلائنٹس کو کالز میں ترجمہ کرتا ہے۔

## ملٹی-موڈل تعامل

جب کہ ہم نے اسپیچ-ٹو-ٹیکسٹ پر توجہ مرکوز کی ہے، حقیقی دنیا کا انسانی-روبوٹ تعامل اکثر **ملٹی-موڈل** ہوتا ہے۔

*   **اسپیچ + اشارہ:** "وہ آبجیکٹ اٹھاؤ" (اشارہ کرتے ہوئے)۔
*   **اسپیچ + ویژن:** "سرخ کتاب تلاش کرو۔" (بصری خصوصیات کا استعمال کرتے ہوئے)۔

LLMs ملٹی-موڈل ان پٹس پر کارروائی کرنے میں تیزی سے زیادہ قابل ہو رہے ہیں، جس سے انہیں ٹیکسٹ، تصاویر، اور ممکنہ طور پر دیگر سینسر اسٹریمز سے معلومات کو فیوز کرنے کی اجازت ملتی ہے تاکہ صارف کے ارادے کی زیادہ مکمل سمجھ بنائی جا سکے۔ یہ **ویژن-لینگویج-ایکشن (VLA)** ماڈلز کا جوہر ہے۔

روبوٹک کنٹرول کا مستقبل صرف کوڈ لکھنے کے بارے میں نہیں ہے، بلکہ طاقتور AI ماڈلز کو انسانی اصطلاحات میں کاموں کے بارے میں استدلال کرنے اور پھر ان خلاصہ منصوبوں کو جسمانی اعمال میں بغیر کسی رکاوٹ کے ترجمہ کرنے کے لیے تربیت اور پرامپٹ کرنے کے بارے میں ہے۔
