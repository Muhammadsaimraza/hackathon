"use strict";(globalThis.webpackChunkrobolearn=globalThis.webpackChunkrobolearn||[]).push([[7957],{4861:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-humanoid-vla/conversational-robotics/index","title":"Chapter 20: Conversational Robotics & VLAs","description":"This chapter brings together the power of Large Language Models (LLMs) with robot perception and action. You will learn to build Vision-Language-Action (VLA) systems that allow a robot to understand natural language commands and translate them into physical actions.","source":"@site/docs/module-4-humanoid-vla/20-conversational-robotics/index.md","sourceDirName":"module-4-humanoid-vla/20-conversational-robotics","slug":"/module-4-humanoid-vla/conversational-robotics/","permalink":"/robolearn/docs/module-4-humanoid-vla/conversational-robotics/","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammadsaimraza/robolearn/tree/main/docs/module-4-humanoid-vla/20-conversational-robotics/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Lesson 19.2: Introduction to MoveIt2","permalink":"/robolearn/docs/module-4-humanoid-vla/motion-planning/19-2-introduction-to-moveit2"},"next":{"title":"Lesson 20.1: The Voice-to-Action Pipeline","permalink":"/robolearn/docs/module-4-humanoid-vla/conversational-robotics/20-1-voice-to-action-pipeline"}}');var i=o(4848),a=o(8453);const s={},r="Chapter 20: Conversational Robotics & VLAs",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Lessons",id:"lessons",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-20-conversational-robotics--vlas",children:"Chapter 20: Conversational Robotics & VLAs"})}),"\n",(0,i.jsxs)(e.p,{children:["This chapter brings together the power of Large Language Models (LLMs) with robot perception and action. You will learn to build ",(0,i.jsx)(e.strong,{children:"Vision-Language-Action (VLA)"})," systems that allow a robot to understand natural language commands and translate them into physical actions."]}),"\n",(0,i.jsx)(e.p,{children:'This is the cutting edge of robotics, where AI\'s ability to reason about the world through language meets its ability to operate within it. You will learn how to build a complete "Voice-to-Action" pipeline, enabling your robot to respond intelligently to human commands.'}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Explain the components of a Voice-to-Action pipeline (Speech-to-Text, LLM Planning, Plan-to-Action)."}),"\n",(0,i.jsx)(e.li,{children:"Use an LLM for cognitive planning, translating natural language into structured robot plans."}),"\n",(0,i.jsx)(e.li,{children:"Understand the challenges and opportunities of multi-modal interaction in robotics."}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"lessons",children:"Lessons"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.strong,{children:"Lesson 20.1: The Voice-to-Action Pipeline"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.strong,{children:"Lesson 20.2: LLM Cognitive Planning"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,o)=>{o.d(e,{R:()=>s,x:()=>r});var t=o(6540);const i={},a=t.createContext(i);function s(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);