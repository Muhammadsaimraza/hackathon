"use strict";(globalThis.webpackChunkrobolearn=globalThis.webpackChunkrobolearn||[]).push([[5594],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var o=t(6540);const i={},s=o.createContext(i);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(s.Provider,{value:n},e.children)}},8867:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-humanoid-vla/conversational-robotics/20-1-voice-to-action-pipeline","title":"Lesson 20.1: The Voice-to-Action Pipeline","description":"The ultimate goal of many robotics applications is to create a robot that can naturally interact with humans, understanding complex instructions and executing them in the physical world. The Voice-to-Action (V2A) Pipeline is the framework for achieving this.","source":"@site/docs/module-4-humanoid-vla/20-conversational-robotics/20-1-voice-to-action-pipeline.md","sourceDirName":"module-4-humanoid-vla/20-conversational-robotics","slug":"/module-4-humanoid-vla/conversational-robotics/20-1-voice-to-action-pipeline","permalink":"/robolearn/docs/module-4-humanoid-vla/conversational-robotics/20-1-voice-to-action-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammadsaimraza/robolearn/tree/main/docs/module-4-humanoid-vla/20-conversational-robotics/20-1-voice-to-action-pipeline.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Chapter 20: Conversational Robotics & VLAs","permalink":"/robolearn/docs/module-4-humanoid-vla/conversational-robotics/"},"next":{"title":"Lesson 20.2: LLM Cognitive Planning","permalink":"/robolearn/docs/module-4-humanoid-vla/conversational-robotics/20-2-llm-cognitive-planning"}}');var i=t(4848),s=t(8453);const r={},a="Lesson 20.1: The Voice-to-Action Pipeline",l={},c=[{value:"Stage 1: Speech-to-Text (STT)",id:"stage-1-speech-to-text-stt",level:2},{value:"Stage 2: LLM Cognitive Planning",id:"stage-2-llm-cognitive-planning",level:2},{value:"Stage 3: Plan-to-Action Execution",id:"stage-3-plan-to-action-execution",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-201-the-voice-to-action-pipeline",children:"Lesson 20.1: The Voice-to-Action Pipeline"})}),"\n",(0,i.jsxs)(n.p,{children:["The ultimate goal of many robotics applications is to create a robot that can naturally interact with humans, understanding complex instructions and executing them in the physical world. The ",(0,i.jsx)(n.strong,{children:"Voice-to-Action (V2A) Pipeline"})," is the framework for achieving this."]}),"\n",(0,i.jsx)(n.p,{children:'The V2A pipeline breaks down the complex task of "understanding a spoken command and acting on it" into a series of manageable stages.'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'graph TD\n    A[Human Spoken Command] --\x3e B{1. Speech-to-Text (STT)};\n    B -- "Text Transcript" --\x3e C{2. LLM Cognitive Planning};\n    C -- "Structured Robot Plan (e.g., JSON)" --\x3e D{3. Plan-to-Action Execution};\n    D -- "ROS 2 Commands (e.g., /cmd_vel, MoveIt goal)" --\x3e E[Robot Physical Action];\n'})}),"\n",(0,i.jsx)(n.h2,{id:"stage-1-speech-to-text-stt",children:"Stage 1: Speech-to-Text (STT)"}),"\n",(0,i.jsx)(n.p,{children:"The first step is to convert the human's spoken words into text that the robot's software can process."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input:"})," Audio from a microphone array (e.g., the ReSpeaker array on our edge kit)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Technology:"})," State-of-the-art speech recognition models. ",(0,i.jsx)(n.strong,{children:"OpenAI Whisper"})," is a prime example of a highly performant and robust model for this task."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"The microphone captures the audio."}),"\n",(0,i.jsx)(n.li,{children:"An STT engine (running locally on the robot or in the cloud) processes the audio."}),"\n",(0,i.jsx)(n.li,{children:"It outputs a text transcript of what was said."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Integration:"})," A ROS 2 node would subscribe to an audio topic, pass the audio to the STT engine, and then publish the resulting text to a ",(0,i.jsx)(n.code,{children:"/transcribed_text"})," topic."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Challenges:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Noise:"})," Real-world environments are noisy, making accurate transcription difficult."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accents/Dialects:"})," STT models need to be robust to a wide variety of human speech patterns."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency:"})," The transcription needs to happen fast enough for a natural conversation."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"stage-2-llm-cognitive-planning",children:"Stage 2: LLM Cognitive Planning"}),"\n",(0,i.jsx)(n.p,{children:"Once we have the text, the next stage is to understand the human's intent and generate a plan of action. This is the domain of Large Language Models (LLMs)."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input:"})," The text transcript from the STT stage."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Technology:"})," Generative AI models like GPT-4, Gemini, or Claude."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'A carefully crafted "prompt" is sent to the LLM. This prompt defines the robot\'s capabilities, the available actions, and the context of the conversation.'}),"\n",(0,i.jsx)(n.li,{children:"The LLM processes the prompt and the human command."}),"\n",(0,i.jsx)(n.li,{children:"It generates a structured, machine-readable plan (often in JSON format) that outlines the steps the robot needs to take."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Integration:"})," A ROS 2 node would subscribe to the ",(0,i.jsx)(n.code,{children:"/transcribed_text"})," topic, formulate the prompt, send it to the LLM API (or a local LLM), and then publish the resulting JSON plan to a ",(0,i.jsx)(n.code,{children:"/robot_plan"})," topic."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Challenges:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grounding:"})," Ensuring the LLM's understanding of the world aligns with the robot's physical capabilities and environment. An LLM might suggest \"fly to the moon,\" but the robot can't do that."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Space:"})," Defining a clear, unambiguous set of actions that the LLM can output and the robot can execute."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety:"})," Preventing the LLM from generating dangerous or inappropriate plans."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"stage-3-plan-to-action-execution",children:"Stage 3: Plan-to-Action Execution"}),"\n",(0,i.jsx)(n.p,{children:"The final stage is to take the structured plan from the LLM and translate it into the low-level ROS 2 commands that control the robot's hardware."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input:"})," The structured robot plan (e.g., JSON) from the LLM."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Technology:"})," ROS 2 Action Clients, Service Clients, and Publishers."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:['A dedicated ROS 2 node (the "Action Executor" or "Task Orchestrator") subscribes to the ',(0,i.jsx)(n.code,{children:"/robot_plan"})," topic."]}),"\n",(0,i.jsx)(n.li,{children:"It parses the plan, step by step."}),"\n",(0,i.jsxs)(n.li,{children:["For each step in the plan, it calls the appropriate ROS 2 action server, service, or publishes to a topic to execute the low-level robot behavior (e.g., calling the Nav2 ",(0,i.jsx)(n.code,{children:"NavigateToPose"})," action server for a ",(0,i.jsx)(n.code,{children:"GOTO"})," command, or sending commands to a MoveIt2 action server for a ",(0,i.jsx)(n.code,{children:"PICKUP"})," command)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Integration:"})," This node would subscribe to ",(0,i.jsx)(n.code,{children:"/robot_plan"})," and act as a client to various other ROS 2 components."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Challenges:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness:"})," Ensuring that each low-level action is executed reliably and handles errors gracefully."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Management:"})," Keeping track of the robot's current state and the progress of the plan."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This pipeline allows us to separate the high-level, human-like reasoning of LLMs from the low-level, real-time control of the robot, creating a powerful and flexible architecture for conversational robotics."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);